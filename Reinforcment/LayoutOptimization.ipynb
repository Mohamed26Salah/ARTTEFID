{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPj380jOj7pu"
   },
   "source": [
    "# 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RoomModel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Opening JSON file\n",
    "f = open('SampleData.json')\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "Room =Room.from_dict(data)\n",
    "  \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107, 198], [102, 60], [50, 50]]\n"
     ]
    }
   ],
   "source": [
    "print(Room.furniture.dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gTgWJgGj7p2"
   },
   "source": [
    "# 1. Test Random Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Uh3jG0Rkj7p3"
   },
   "outputs": [],
   "source": [
    "from Room import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YzVtE_KXj7p4"
   },
   "outputs": [],
   "source": [
    "# RoomEnv(Furniture Dimension ,Doors (doors Dimensions , (Door1 pos, Door2 pos,etc )),Windows(windows Dimensions,(window1 pos, window2 pos,etc )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8ZxpAR8Rj7p5"
   },
   "outputs": [],
   "source": [
    "env = RoomEnv(Room.furniture.dimensions,Room.doors.doors_dimensions,Room.doors.doors_positions,Room.windows.windows_dimensions,Room.windows.windows_positions,Room.furniture.keywords, Room.room_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mFoo5gxj7p7",
    "outputId": "0c91ddac-c86d-4637-d77e-c20047d66adf",
    "tags": [
     "nbconvert"
    ]
   },
   "outputs": [],
   "source": [
    "# episodes = 10\n",
    "# for episode in range(1, episodes+1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0 \n",
    "#     switch = False\n",
    "    \n",
    "#     while not done:\n",
    "#         env.render(\"human\")\n",
    "#         if(switch):\n",
    "#             action = 0\n",
    "#             switch=False\n",
    "#         else:\n",
    "#             action = 2\n",
    "#             switch=True\n",
    "# #         print('CurrentPos:{}'.format(action))\n",
    "        \n",
    "#         n_state, reward, done, info = env.step(action)\n",
    "#         score+=reward\n",
    "#     print('Episode:{} Score:{} '.format(episode, score))\n",
    "#     env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import  BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhTt3XNij7p8"
   },
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RvSScRXcj7p9"
   },
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import  *\n",
    "from rl.memory import SequentialMemory\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten,Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yFV4_QWSj7p-"
   },
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0XBuhvzj7p-",
    "outputId": "92f6224b-197a-4974-85eb-e5ff75c52a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "4\n",
      "(array([117, 241]), 871, False, {})\n",
      "(array([118, 241]), 870, False, {})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(states)\n",
    "print(actions)\n",
    "print(env.step(0))\n",
    "print(env.step(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QPXP6OWAj7p-"
   },
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=states))  \n",
    "    model.add(Dense(128, activation='relu', input_shape=states))\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_zw97GX7j7p_"
   },
   "outputs": [],
   "source": [
    "# del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6Tpf6H_0j7p_"
   },
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtVi0tOXj7p_",
    "outputId": "cc281261-3c8c-47d5-fa0c-adf260b96441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 17,412\n",
      "Trainable params: 17,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(None, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "pt_depth = model.layers[0].get_input_shape_at(node_index=0)\n",
    "print(pt_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XKacIXej7qA"
   },
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U8kDaoVIj7qA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118 241]\n"
     ]
    }
   ],
   "source": [
    "print(env.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107, 198], [102, 60], [50, 50]]\n"
     ]
    }
   ],
   "source": [
    "print(env.furniture_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RsfBpb9ej7qA"
   },
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy =  EpsGreedyQPolicy(eps=0.5)\n",
    "    memory = SequentialMemory(limit=10000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=300, target_model_update= 1e-3)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitall(steps, dqns):\n",
    "  for i in range(steps):\n",
    "    for dqn in dqns:\n",
    "        dqn.fit(env, nb_steps=steps, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "sYl8cAgfj7qB",
    "outputId": "d5dba474-786d-4a66-9e01-2aacb71e595d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From f:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "10000/10000 [==============================] - 45s 4ms/step - reward: 1376.9124\n",
      "done, took 44.899 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=10000, visualize=True, verbose=1)\n",
    "\n",
    "# fitall(100000,[dqn2,dqn])\n",
    "env.close(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 821.3132\n",
      "done, took 43.943 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cbde0069a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nextPlease()\n",
    "dqn2 = build_agent(model, actions)\n",
    "dqn2.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn2.fit(env, nb_steps=10000, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 45s 5ms/step - reward: -0.1774\n",
      "done, took 45.172 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cbde1bb610>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nextPlease()\n",
    "dqn3 = build_agent(model, actions)\n",
    "dqn3.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn3.fit(env, nb_steps=10000, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([129, 388]), array([ 57, 260]), array([400, 462])]\n"
     ]
    }
   ],
   "source": [
    "print(env.state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tteRF-rBj7qE"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H%M%S\")\n",
    "dqn.save_weights('lastSaved2', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dqb0nLAsj7qB",
    "outputId": "a9b643a5-2357-463e-a9c4-0ce1e261eecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scores \u001b[39m=\u001b[39m dqn\u001b[39m.\u001b[39;49mtest(env, nb_episodes\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, visualize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      2\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(scores\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mepisode_reward\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\rl\\core.py:341\u001b[0m, in \u001b[0;36mAgent.test\u001b[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m    339\u001b[0m     callbacks\u001b[39m.\u001b[39mon_step_begin(episode_step)\n\u001b[1;32m--> 341\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(observation)\n\u001b[0;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mprocess_action(action)\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\rl\\agents\\dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[0;32m    222\u001b[0m     \u001b[39m# Select an action.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mget_recent_state(observation)\n\u001b[1;32m--> 224\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_q_values(state)\n\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    226\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mselect_action(q_values\u001b[39m=\u001b[39mq_values)\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\rl\\agents\\dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_q_values\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m---> 68\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_batch_q_values([state])\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m     69\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions,)\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\rl\\agents\\dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_batch_q_values\u001b[39m(\u001b[39mself\u001b[39m, state_batch):\n\u001b[0;32m     62\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[1;32m---> 63\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_on_batch(batch)\n\u001b[0;32m     64\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mlen\u001b[39m(state_batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions)\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:1214\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1211\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(inputs)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_predict_function()\n\u001b[1;32m-> 1214\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(inputs)\n\u001b[0;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(outputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1217\u001b[0m   \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3824\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3818\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callable_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m feed_arrays \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_arrays \u001b[39mor\u001b[39;00m\n\u001b[0;32m   3819\u001b[0m     symbol_vals \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_symbol_vals \u001b[39mor\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m     feed_symbols \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_symbols \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetches \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches \u001b[39mor\u001b[39;00m\n\u001b[0;32m   3821\u001b[0m     session \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session):\n\u001b[0;32m   3822\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 3824\u001b[0m fetched \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callable_fn(\u001b[39m*\u001b[39;49marray_vals,\n\u001b[0;32m   3825\u001b[0m                             run_metadata\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_metadata)\n\u001b[0;32m   3826\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches):])\n\u001b[0;32m   3827\u001b[0m output_structure \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   3828\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_structure,\n\u001b[0;32m   3829\u001b[0m     fetched[:\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)],\n\u001b[0;32m   3830\u001b[0m     expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1470\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1469\u001b[0m   run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1470\u001b[0m   ret \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39;49mTF_SessionRunCallable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_session,\n\u001b[0;32m   1471\u001b[0m                                          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, args,\n\u001b[0;32m   1472\u001b[0m                                          run_metadata_ptr)\n\u001b[0;32m   1473\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1474\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "scores = dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()\n",
    "print(scores.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "u3Y3hSwHHibv",
    "outputId": "85f35a85-be28-49b8-fcd5-175b5796aa23"
   },
   "outputs": [],
   "source": [
    "print(env.colided)\n",
    "env.step(0)\n",
    "print(env.colided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8OzdBNwj7qF"
   },
   "outputs": [],
   "source": [
    "dqn.load_weights('sequential_1doorWindowRandomStartSuccessdqn_weights2.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dqb0nLAsj7qB",
    "outputId": "a9b643a5-2357-463e-a9c4-0ce1e261eecf"
   },
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close();\n",
    "print(scores.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d8uez7dj7qD"
   },
   "source": [
    "# 4. Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZH4dqugBj7qE"
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fz7RAJ9Rj7qE"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIM5k-M2j7qF"
   },
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssrGKaxzj7qG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "reinforcmentlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b16aa9c2f9ca519bd84c3170bdda05fa03e46a681e2095a6220d679a6ba17b2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
