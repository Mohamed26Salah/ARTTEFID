{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPj380jOj7pu"
   },
   "source": [
    "# 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.8385584354400635, 1]\n",
      "[3.5821986198425293, 1]\n",
      "[3.449700355529785, 1]\n",
      "[3.093371629714966, 1]\n",
      "[0.7451848387718201, 1]\n",
      "[0.1324978768825531, 1]\n"
     ]
    }
   ],
   "source": [
    "from FromNewToOld import Room2,wallsArray2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gTgWJgGj7p2"
   },
   "source": [
    "# 1. Test Random Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Uh3jG0Rkj7p3"
   },
   "outputs": [],
   "source": [
    "from Room import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YzVtE_KXj7p4"
   },
   "outputs": [],
   "source": [
    "# RoomEnv(Furniture Dimension ,Doors (doors Dimensions , (Door1 pos, Door2 pos,etc )),Windows(windows Dimensions,(window1 pos, window2 pos,etc )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8ZxpAR8Rj7p5"
   },
   "outputs": [],
   "source": [
    "env = RoomEnv(Room2.furniture.dimensions,Room2.doors.doors_dimensions,Room2.doors.doors_positions,Room2.windows.windows_dimensions,Room2.windows.windows_positions,Room2.furniture.keywords, Room2.room_dimensions,wallsArray2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mFoo5gxj7p7",
    "outputId": "0c91ddac-c86d-4637-d77e-c20047d66adf",
    "tags": [
     "nbconvert"
    ]
   },
   "outputs": [],
   "source": [
    "# episodes = 10\n",
    "# for episode in range(1, episodes+1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0 \n",
    "#     switch = False\n",
    "    \n",
    "#     while not done:\n",
    "#         env.render(\"human\")\n",
    "#         if(switch):\n",
    "#             action = 0\n",
    "#             switch=False\n",
    "#         else:\n",
    "#             action = 2\n",
    "#             switch=True\n",
    "# #         print('CurrentPos:{}'.format(action))\n",
    "        \n",
    "#         n_state, reward, done, info = env.step(action)\n",
    "#         score+=reward\n",
    "#     print('Episode:{} Score:{} '.format(episode, score))\n",
    "#     env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import  BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhTt3XNij7p8"
   },
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RvSScRXcj7p9"
   },
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import  *\n",
    "from rl.memory import SequentialMemory\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten,Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yFV4_QWSj7p-"
   },
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0XBuhvzj7p-",
    "outputId": "92f6224b-197a-4974-85eb-e5ff75c52a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "4\n",
      "(array([210, 238]), 847, False, {})\n",
      "(array([211, 238]), 848, False, {})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(states)\n",
    "print(actions)\n",
    "print(env.step(0))\n",
    "print(env.step(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QPXP6OWAj7p-"
   },
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=states))  \n",
    "    model.add(Dense(128, activation='relu', input_shape=states))\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_zw97GX7j7p_"
   },
   "outputs": [],
   "source": [
    "# del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6Tpf6H_0j7p_"
   },
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtVi0tOXj7p_",
    "outputId": "cc281261-3c8c-47d5-fa0c-adf260b96441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 17,412\n",
      "Trainable params: 17,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(None, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "pt_depth = model.layers[0].get_input_shape_at(node_index=0)\n",
    "print(pt_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XKacIXej7qA"
   },
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "U8kDaoVIj7qA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[211 238]\n"
     ]
    }
   ],
   "source": [
    "print(env.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107, 198], [107, 198], [49, 49], [102, 60], [50, 50]]\n"
     ]
    }
   ],
   "source": [
    "print(env.furniture_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RsfBpb9ej7qA"
   },
   "outputs": [],
   "source": [
    "def build_agent(model, actions, policy):\n",
    "    policy =  policy\n",
    "    memory = SequentialMemory(limit=30000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=300, target_model_update= 1e-3)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitall(steps, dqns):\n",
    "  for i in range(steps):\n",
    "    for dqn in dqns:\n",
    "        dqn.fit(env, nb_steps=steps, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "sYl8cAgfj7qB",
    "outputId": "d5dba474-786d-4a66-9e01-2aacb71e595d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From f:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "[[1.0332562923431396, -0.18603730201721191], [1.1861575841903687, 2.4347047805786133], [-1.5460700988769531, -0.2623029947280884], [-1.169244647026062, 2.663517475128174], [-2.4667797088623047, 1.2430967092514038], [-2.2428553104400635, 1.5481748580932617]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dqn \u001b[39m=\u001b[39m build_agent(model, actions , EpsGreedyQPolicy(eps\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m))\n\u001b[0;32m      2\u001b[0m dqn\u001b[39m.\u001b[39mcompile(Adam(lr\u001b[39m=\u001b[39m\u001b[39m1e-1\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m dqn\u001b[39m.\u001b[39;49mfit(env, nb_steps\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m, visualize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# fitall(100000,[dqn2,dqn])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\rl\\core.py:186\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    184\u001b[0m         accumulated_info[key] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(value)\n\u001b[0;32m    185\u001b[0m     accumulated_info[key] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m value\n\u001b[1;32m--> 186\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_action_end(action)\n\u001b[0;32m    187\u001b[0m reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[0;32m    188\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\rl\\callbacks.py:98\u001b[0m, in \u001b[0;36mCallbackList.on_action_end\u001b[1;34m(self, action, logs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m callable(\u001b[39mgetattr\u001b[39m(callback, \u001b[39m'\u001b[39m\u001b[39mon_action_end\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)):\n\u001b[1;32m---> 98\u001b[0m         callback\u001b[39m.\u001b[39;49mon_action_end(action, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mf:\\Programs\\anaconda3\\envs\\reinforcmentlearning\\lib\\site-packages\\rl\\callbacks.py:360\u001b[0m, in \u001b[0;36mVisualizer.on_action_end\u001b[1;34m(self, action, logs)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_action_end\u001b[39m(\u001b[39mself\u001b[39m, action, logs):\n\u001b[0;32m    359\u001b[0m     \u001b[39m\"\"\" Render environment at the end of each action \"\"\"\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\SIGMA\\Documents\\UOTTEFID\\Reinforcment\\Room.py:322\u001b[0m, in \u001b[0;36mRoomEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock\u001b[39m.\u001b[39mtick(\u001b[39m200\u001b[39m)\n\u001b[0;32m    321\u001b[0m         \u001b[39m# need to draw rects dynamically in a func \u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDrawElements()\n\u001b[0;32m    323\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolor[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mturn]\u001b[39m=\u001b[39m(\u001b[39m252\u001b[39m, \u001b[39m198\u001b[39m, \u001b[39m108\u001b[39m)\n\u001b[0;32m    324\u001b[0m \u001b[39m#         pygame.event.pump()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SIGMA\\Documents\\UOTTEFID\\Reinforcment\\Room.py:338\u001b[0m, in \u001b[0;36mRoomEnv.DrawElements\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate3)):\n\u001b[0;32m    337\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate3)\n\u001b[1;32m--> 338\u001b[0m     pygame\u001b[39m.\u001b[39mdraw\u001b[39m.\u001b[39mrect(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen,(\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m),((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate3[i][\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_factor\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m\u001b[39m-\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwalls_dimensions[i][\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)),(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate3[i][\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_factor\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m\u001b[39m-\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwalls_dimensions[i][\u001b[39m1\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)),\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwalls_dimensions[i][\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwalls_dimensions[i][\u001b[39m2\u001b[39;49m]\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m)) \n\u001b[0;32m    340\u001b[0m \u001b[39m# for i in range(len(self.keywords)):\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[39m#     pygame.draw.rect(self.screen,(0, 50, 0),((self.BestStates[i][0]*self.scale_factor-(self.furniture_dimensions[i][0]/2)),(self.BestStates[i][1]*self.scale_factor-(self.furniture_dimensions[i][1]/2)),self.furniture_dimensions[i][0],self.furniture_dimensions[i][1])) \u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[39m#     self.screen.blit(self.Furnituretext[i], (self.BestStates[i][0]*self.scale_factor, self.BestStates[i][1]*self.scale_factor))\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeywords)):\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn = build_agent(model, actions , EpsGreedyQPolicy(eps=0.2))\n",
    "dqn.compile(Adam(lr=1e-1), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=20000, visualize=True, verbose=1)\n",
    "\n",
    "# fitall(100000,[dqn2,dqn])\n",
    "env.close(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.nextPlease()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = build_model(states, actions)\n",
    "dqn2 = build_agent(model2, actions, EpsGreedyQPolicy(eps=0.1))\n",
    "dqn2.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn2.fit(env, nb_steps=30000, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model(states, actions)\n",
    "env.nextPlease()\n",
    "dqn3 = build_agent(model2, actions, EpsGreedyQPolicy(eps=0.1))\n",
    "dqn3.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn3.fit(env, nb_steps=300, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model(states, actions)\n",
    "env.nextPlease()\n",
    "dqn4 = build_agent(model2, actions, EpsGreedyQPolicy(eps=0.5))\n",
    "dqn4.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn4.fit(env, nb_steps=20000, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.nextPlease()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.step(0))\n",
    "env.move_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = build_model(states, actions)\n",
    "dqn5 = build_agent(model3, actions, EpsGreedyQPolicy(eps=0.1))\n",
    "dqn5.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn5.fit(env, nb_steps=30000, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tteRF-rBj7qE"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H%M%S\")\n",
    "dqn.save_weights('lastSaved2', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dqb0nLAsj7qB",
    "outputId": "a9b643a5-2357-463e-a9c4-0ce1e261eecf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# scores = dqn.test(env, nb_episodes=5, visualize=True)\n",
    "# env.close()\n",
    "# print(scores.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "u3Y3hSwHHibv",
    "outputId": "85f35a85-be28-49b8-fcd5-175b5796aa23"
   },
   "outputs": [],
   "source": [
    "print(env.colided)\n",
    "env.step(0)\n",
    "print(env.colided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8OzdBNwj7qF"
   },
   "outputs": [],
   "source": [
    "dqn.load_weights('sequential_1doorWindowRandomStartSuccessdqn_weights2.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dqb0nLAsj7qB",
    "outputId": "a9b643a5-2357-463e-a9c4-0ce1e261eecf"
   },
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close();\n",
    "print(scores.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d8uez7dj7qD"
   },
   "source": [
    "# 4. Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZH4dqugBj7qE"
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fz7RAJ9Rj7qE"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIM5k-M2j7qF"
   },
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssrGKaxzj7qG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "reinforcmentlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b16aa9c2f9ca519bd84c3170bdda05fa03e46a681e2095a6220d679a6ba17b2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
